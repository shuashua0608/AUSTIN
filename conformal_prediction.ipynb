{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a1b28a0-73f0-463d-8800-64291ab2b20c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd51cc5-9f25-4d19-b065-874ff3c1e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conformal Prediction Helper Functions\n",
    "def compute_nonconformity(model, X_val, y_val, alpha=0.8):\n",
    "    \"\"\"Compute nonconformity scores using MRI (ground truth) and weighted triage labels.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.tensor(X_val, dtype=torch.float32))\n",
    "        probs = torch.softmax(logits, dim=1).numpy()\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        \n",
    "        # Nonconformity score: (weighted difference from MRI and triage labels)\n",
    "        scores = alpha * np.abs(probs[:, 1] - y_val[:, 0]) + (1 - alpha) * np.abs(probs[:, 1] - y_val[:, 1])\n",
    "    return scores\n",
    "\n",
    "def calibrate_conformal_threshold(scores, quantile=0.95):\n",
    "    \"\"\"Determine the threshold from calibration scores.\"\"\"\n",
    "    return np.quantile(scores, quantile)\n",
    "\n",
    "def predict_with_conformal(model, X_test, threshold):\n",
    "    \"\"\"Predict with conformal uncertainty bounds.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.tensor(X_test, dtype=torch.float32))\n",
    "        probs = torch.softmax(logits, dim=1).numpy()\n",
    "    \n",
    "    # Generate prediction sets based on conformal threshold\n",
    "    prediction_sets = [\n",
    "        {\"Stroke\"} if p[1] > threshold else {\"Stroke\", \"No-Stroke\"}\n",
    "        for p in probs\n",
    "    ]\n",
    "    return prediction_sets, np.argmax(probs, axis=1), probs\n",
    "\n",
    "# Load Data (Placeholder: Replace with actual extracted features)\n",
    "X_train, y_train = np.random.rand(100, 4096), np.random.randint(0, 2, (100, 2))  # Early-stage data\n",
    "X_val, y_val = np.random.rand(50, 4096), np.random.randint(0, 2, (50, 2))  # Middle-stage calibration data\n",
    "X_test, y_test = np.random.rand(30, 4096), np.random.randint(0, 2, (30,))  # Newest-stage test data\n",
    "\n",
    "# Train Model\n",
    "model = StrokeClassifier(input_dim=4096)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, X_train, y_train, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(torch.tensor(X_train, dtype=torch.float32))\n",
    "        loss = criterion(logits, torch.tensor(np.argmax(y_train, axis=1), dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "train_model(model, X_train, y_train)\n",
    "\n",
    "# Compute nonconformity scores & calibrate threshold\n",
    "nonconformity_scores = compute_nonconformity(model, X_val, y_val)\n",
    "threshold = calibrate_conformal_threshold(nonconformity_scores)\n",
    "\n",
    "# Predict on new cases with conformal confidence sets\n",
    "conformal_predictions, predicted_labels, predicted_probs = predict_with_conformal(model, X_test, threshold)\n",
    "\n",
    "# Evaluate Model\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "f1 = f1_score(y_test, predicted_labels)\n",
    "recall = recall_score(y_test, predicted_labels)\n",
    "conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
    "\n",
    "# Conformal Prediction Metrics\n",
    "def compute_coverage(prediction_sets, y_test):\n",
    "    \"\"\"Compute coverage: fraction of times the true label is in the prediction set.\"\"\"\n",
    "    correct = sum(y_test[i] in pred_set for i, pred_set in enumerate(prediction_sets))\n",
    "    return correct / len(y_test)\n",
    "\n",
    "def compute_efficiency(prediction_sets):\n",
    "    \"\"\"Compute efficiency: average size of the prediction sets.\"\"\"\n",
    "    return np.mean([len(pred_set) for pred_set in prediction_sets])\n",
    "\n",
    "coverage = compute_coverage(conformal_predictions, y_test)\n",
    "efficiency = compute_efficiency(conformal_predictions)\n",
    "\n",
    "# Output Results\n",
    "for i, pred_set in enumerate(conformal_predictions[:5]):\n",
    "    print(f\"Test Case {i+1}: Prediction Set = {pred_set}\")\n",
    "\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"\\nConformal Prediction Evaluation:\")\n",
    "print(f\"Coverage: {coverage:.4f}\")\n",
    "print(f\"Efficiency (Avg. Prediction Set Size): {efficiency:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSV2",
   "language": "python",
   "name": "dsv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
